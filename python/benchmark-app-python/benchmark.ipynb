{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Application\n",
    "\n",
    "This is a sample reference implementation to showcase object detection (car in this case) with single-shot detection (SSD) and Async API.\n",
    "Async API improves the overall frame-rate of the application by not waiting for the inference to complete but continuing to do things on the host while inference accelerator is busy. \n",
    "Specifically, this code demonstrates two parallel inference requests by processing the current frame while the next input frame is being captured. This essentially hides the latency of frame capture.\n",
    "\n",
    "This is a sample reference implementation to demonstrates how to run the Benchmark Application demo. It performs inference of convolutional neural networks (CNNs). The user has ability to compare different options of the inference engine: synchronous vs asynchronous API, number of threads, batch size, number of inference requests, etc.\n",
    "\n",
    "For synchronous mode, the primary metric is latency. The application creates one inference request and executes it. During the execution, the application collects two types of metrics: latency for each inference request, and duration of all executions. Reported latency value is calculated as mean value of all collected latencies. Reported throughput value is a derivative from reported latency and additionally depends on batch size.\n",
    "\n",
    "For asynchronous mode, the primary metric is throughput in frames per second (FPS). The application creates a certain number of inference requests and starts them. The inference requests are executed asynchronously. The application measures all inference requests executions and reports the throughput metric based on batch size and total execution duration.\n",
    "\n",
    "## Overview of How It Works\n",
    "\n",
    "At start-up, the sample application reads the equivalent of command line arguments and loads a network to the Inference Engine (IE) plugin. \n",
    "A job is submitted to the hardware accelerator (Intel® Core CPU, Intel® HD Graphics GPU, Intel® Core CPU, Intel® Movidius™ Neural Compute Stick, Intel® Neural Compute Stick 2 and Intel® HDDL-R). The inference runs for a pre-defined interval, typically 60 seconds. After the inference is completed, statistics are displayed to summarize the performance.\n",
    "\n",
    "## Step 0: Set Up\n",
    "\n",
    "Run the below cell to import Python dependencies needed for displaying the results in this notebook\n",
    "(tip: select the cell and use **Ctrl+enter** to run the cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path().resolve().parent.parent))\n",
    "from demoTools.demoutils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Using OpenVINO\n",
    "\n",
    "First, let's try running inference on a single image to see how OpenVINO works.\n",
    "We will be using OpenVINO's Inference Engine (IE) to run SqueezeNet image classification. \n",
    "There are five steps involved in this task:\n",
    "\n",
    "1. Create a Intermediate Representation (IR) Model using the Intel Model Optimizer\n",
    "2. Choose a device and create IEPlugin for the device\n",
    "3. Read the IRModel using IENetwork\n",
    "4. Load the IENetwork into the Plugin\n",
    "5. Run inference.\n",
    "\n",
    "### 1.1 Creating IR Model\n",
    "\n",
    "Intel Model Optimizer creates Intermediate Representation (IR) models that are optimized for different end-point target devices.\n",
    "These models can be created from existsing DNN models from popular frameworks (e.g. Caffe, TF) using the Intel Model Optimizer. \n",
    "\n",
    "The Intel Distribution of OpenVINO includes a utility script `model_downloader.py` that you can use to download some common modes. Run the following cell to see the models available through `model_downloader.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/computer_vision_sdk/deployment_tools/model_downloader/downloader.py --print_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** the '!' is a special Jupyter Notebook command that allows you to run shell commands as if you are in commannd line. So the above command will work straight out of the box on in a terminal (with '!' removed).\n",
    "\n",
    "In this demo, we will be using the **squeezenet1.1** model. This model can be downloaded with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/computer_vision_sdk/deployment_tools/model_downloader/downloader.py --name squeezenet1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command downloads the model in the directory `classification`, with the `.caffemodel` located at `classification/squeezenet/1.1/caffe/squeezenet1.1.caffemodel`\n",
    "\n",
    "Now, let's convert this to the optimized model using the model optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/computer_vision_sdk/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model classification/squeezenet/1.1/caffe/squeezenet1.1.caffemodel \\\n",
    "--data_type FP32 \\\n",
    "--output_dir ir/FP32/squeezenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the arguments are:\n",
    "* `--input-model`: the original model\n",
    "* `--data_type`: Data type to use. One of {FP32, FP16, half, float}\n",
    "* `--output_dir`: Directory that stores the generated IR.\n",
    "\n",
    "This script also supports `-h` that will you can get the full list of arguments.\n",
    "\n",
    "There are two files produced:\n",
    "```\n",
    "classification/squeezenet/1.1/caffe/squeezenet1.1.xml\n",
    "classification/squeezenet/1.1/caffe/squeezenet1.1.bin\n",
    "```\n",
    "These will be used later in the exercise.\n",
    "\n",
    "We will also be needing the FP16 version of the model for the calculations on the MYRIAD or GPU architectures. Run the following cell to create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/computer_vision_sdk/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model classification/squeezenet/1.1/caffe/squeezenet1.1.caffemodel \\\n",
    "--data_type FP16 \\\n",
    "--output_dir ir/FP16/squeezenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Choosing Device\n",
    "\n",
    "Now we must select the device used for inferencing. This is done by loading the appropriate plugin to initialize the specified device and load the extensions library (if specified) provided in the extension/ folder for the device.\n",
    "\n",
    "\n",
    "The following cell constructs **`IEPlugin`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.inference_engine import IEPlugin\n",
    "\n",
    "cpu_ext = '../../Tutorials/classification/libcpu_extension.so'\n",
    "\n",
    "def createPlugin(device, extension_list):\n",
    "    # Plugin initialization for specified device. We will be targeting CPU initially.\n",
    "    plugin = IEPlugin(device=device)\n",
    "\n",
    "    # Loading additional exension libraries for the CPU\n",
    "    for extension in extension_list:\n",
    "        plugin.add_cpu_extension(cpu_ext)\n",
    "    \n",
    "    return plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "Currently, three types of plugins are supported: CPU, GPU, and MYRIAD. CPU plugin may require additional extensions to improve performance, `add_cpu_extension` function is used to load these additional extensions.\n",
    "\n",
    "\n",
    "### 1.3 Read the IR (Intermediate Representation) model\n",
    "\n",
    "We can import optimized models (weights) from step 1.1 into our neural network using **`IENetwork`**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.inference_engine import IENetwork\n",
    "\n",
    "def createNetwork(model_xml, model_bin, plugin):\n",
    "    # Importing network weights from IR models.\n",
    "    net = IENetwork(model=model_xml, weights=model_bin)\n",
    "    \n",
    "    # Some layers in IR models may be unsupported by some plugins. \n",
    "    if \"CPU\" in plugin.device:\n",
    "        supported_layers = plugin.get_supported_layers(net)\n",
    "        not_supported_layers = [l for l in net.layers.keys() if l not in supported_layers]\n",
    "        if len(not_supported_layers) != 0:\n",
    "            print(\"Following layers are not supported by the plugin for specified device {}:\\n {}\".\n",
    "                      format(plugin.device, ', '.join(not_supported_layers)))\n",
    "            print(\"Please try to specify cpu extensions library path in sample's command line parameters \"\n",
    "                  \"using -l or --cpu_extension command line argument\")\n",
    "            return None\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "Some models may be incompatible with some target devices. For example, some types of neural network layers are not supported on the CPU target. \n",
    "\n",
    "### 1.4 Load the network into the plugin\n",
    "\n",
    "Once we have the plugin and the network, we can load the network into the plugin using **`plugin.load`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadNetwork(plugin, net):\n",
    "    # Loading IR model to the plugin.\n",
    "    exec_net = plugin.load(network=net, num_requests=2)\n",
    "    \n",
    "    # Getting the input and outputs of the network\n",
    "    input_blob = next(iter(net.inputs))\n",
    "    output_blob = next(iter(net.outputs))\n",
    "    return exec_net, input_blob, output_blob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Run inference\n",
    "\n",
    "Now we are ready to try running the inference workload using the plugin.\n",
    "First let's load the image using OpenCV.\n",
    "We will also have to do some shape manipulation to convert the image to a format that is compatible with our network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def preprocessImage(img_path, net, input_blob):\n",
    "    # Reading the frame from an image file\n",
    "    frame = cv2.imread(img_path)\n",
    "    \n",
    "    # Reshaping data\n",
    "    n, c, h, w = net.inputs[input_blob].shape\n",
    "    in_frame = cv2.resize(frame, (w, h))\n",
    "    in_frame = in_frame.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n",
    "    return in_frame.reshape((n, c, h, w)),frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the inference, we will be running in **async_mode** by using `start_async` method. \n",
    "With the async_mode, the inference is started in parallel on either a separate thread or device.\n",
    "In other words, `start_async` is non-blocking and the main process is free to do any additional processing needed.\n",
    "\n",
    "During asynchronous runs, the different images are tracked by an integer `request_id`. \n",
    "Because we only have one image to process, we will just use 0.\n",
    "\n",
    "After the inference request executed, check out this <a href=\"https://en.wikipedia.org/wiki/Magpie\">article</a>. Is the classification correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Request id to keep track of\n",
    "def runInference():\n",
    "    plugin = createPlugin(device='CPU', extension_list=[cpu_ext])\n",
    "\n",
    "    model_xml = \"ir/FP32/squeezenet/squeezenet1.1.xml\"\n",
    "    model_bin = \"ir/FP32/squeezenet/squeezenet1.1.bin\"\n",
    "    \n",
    "    net = createNetwork(model_xml, model_bin, plugin)\n",
    "    exec_net, input_blob, out_blob = loadNetwork(plugin, net)\n",
    "    \n",
    "    input_image = '../../Tutorials/classification/bird.jpg'\n",
    "    in_frame, original_frame = preprocessImage(input_image, net, input_blob)\n",
    "    \n",
    "    my_request_id = 0\n",
    "\n",
    "    # Starting the inference in async mode, which starts the inference in parallel\n",
    "    exec_net.start_async(request_id=my_request_id, inputs={input_blob: in_frame})\n",
    "\n",
    "    # Here you can do additional processing or latency masking while we wait\n",
    "\n",
    "    # Blocking wait for a particular request_id\n",
    "    if exec_net.requests[my_request_id].wait(-1) == 0:\n",
    "        # getting the result of the network\n",
    "        res = exec_net.requests[my_request_id].outputs[out_blob]\n",
    "        \n",
    "        labels = 'imagenet1000_clsidx_to_labels.txt'\n",
    "        number_top = 5\n",
    "\n",
    "        if labels:\n",
    "            with open(labels, 'r') as f:\n",
    "                labels_map = [x.split(sep=' ', maxsplit=1)[-1].strip() for x in f]\n",
    "        else:\n",
    "            labels_map = None\n",
    "    \n",
    "        for i, probs in enumerate(res):\n",
    "            probs = np.squeeze(probs)\n",
    "            top_ind = np.argsort(probs)[-number_top:][::-1]\n",
    "            print(\"Image {} is classified:\\n\".format(input_image))\n",
    "            for id in top_ind:\n",
    "                det_label = labels_map[id] if labels_map else \"#{}\".format(id)\n",
    "                print(\"{:.4f} {:<5}\".format(probs[id], det_label))\n",
    "            print(\"\\n\")\n",
    "    else:\n",
    "        print(\"There was an error with the request\")\n",
    "\n",
    "runInference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Inference Throughput\n",
    "\n",
    "Now that we know how to run inference on a single frame, let's evaluate the throughput of the classification model. This part is already implemented in <a href=\"benchmark.py\">benchmark.py</a>. The code on pure throughput, rather than the actual model result. Thus, the script is generic and you are free to supply any model here, not just classificaiton. This include custom models not provided by OpenVINO toolkit.\n",
    "\n",
    "**Command line arguments options and how they are interpreted in the application source code**\n",
    "\n",
    "```\n",
    "SAMPLEPATH=\"/data/reference-sample-data\"\n",
    "python3 benchmark.py -m ${SAMPLEPATH}/ir/$3/squeezenet/squeezenet1.1.xml \\\n",
    "                     -o $1 \\\n",
    "                     -d $2 \\\n",
    "                     -l ${SAMPLEPATH}/extension/libcpu_extension.so\n",
    "```\n",
    "##### The description of the arguments used in the argument parser is the command line executable equivalent.\n",
    "* `-m` location of the pre-trained model which has been pre-processed using the **model optimizer**. There is automated support built in this argument to support both FP32 and FP16 models targeting different hardware.\n",
    "* `-o` location where the output file with inference needs to be stored (results/core, results/xeon, results/gpu, etc.).\n",
    "* `-d` Type of Hardware Acceleration (CPU, GPU, MYRIAD)\n",
    "* `-l` Absolute path to the shared library and is currently optimized for core/xeon (extension/libcpu_extension.so )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creating job file\n",
    "\n",
    "All the code up to this point has been run within the Jupyter Notebook instance running on a development node based on an Intel Xeon Scalable processor, where the Notebook is allocated on a single core. \n",
    "Now we will run the workload on several edge compute nodes represented in the IoT DevCloud. We will send work to the edge compute nodes by submitting the corresponding non-interactive jobs into a queue. For each job, we will specify the type of the edge compute server that must be allocated for the job.\n",
    "\n",
    "The job file is written in Bash, and will be executed directly on the edge compute node.\n",
    "For this example, we have written the job file for you in the notebook.\n",
    "Run the following cell to write this in to the file \"benchmark_job.sh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile benchmark_job.sh\n",
    "\n",
    "# The default path for the job is your home directory, so we change directory to where the files are.\n",
    "cd $PBS_O_WORKDIR\n",
    "\n",
    "# Benchmark script writes output to a file inside a directory. We make sure that this directory exists.\n",
    "# The output directory is the first argument of the bash script. Then we specify the precision of the model.\n",
    "# For the moment, only FP32 and FP16 are possible option, but later on we'll have INT8 as well.\n",
    "# Finally, we specify the number of parallel inference request. This is an important properly for\n",
    "# CPU and MYRIAD/HDDL plug-ins.\n",
    "OUTPUT_FILE=$1\n",
    "DEVICE=$2\n",
    "FP_MODEL=$3\n",
    "NUM_INFER_REQ=$4\n",
    "\n",
    "# Environment variables and compilation for edge compute nodes with FPGAs\n",
    "if [ \"$2\" = \"HETERO:FPGA,CPU\" ]; then\n",
    "    export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/opt/altera/aocl-pro-rte/aclrte-linux64/\n",
    "    source /opt/fpga_support_files/setup_env.sh\n",
    "    aocl program acl0 /opt/intel/computer_vision_sdk/bitstreams/a10_vision_design_bitstreams/5-0_PL1_FP11_SqueezeNet.aocx\n",
    "fi\n",
    "    \n",
    "# Running the benchmark code\n",
    "SAMPLEPATH=$PBS_O_WORKDIR\n",
    "python3 benchmark.py -m ${SAMPLEPATH}/ir/${FP_MODEL}/squeezenet/squeezenet1.1.xml \\\n",
    "                     -o $OUTPUT_FILE \\\n",
    "                     -d $DEVICE \\\n",
    "                     -nireq $NUM_INFER_REQ\n",
    "                     -l /opt/intel/computer_vision_sdk/deployment_tools/inference_engine/samples/build/intel64/Release/lib/libcpu_extension.so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Understand how jobs are submitted into the queue\n",
    "\n",
    "Now that we have the job script, we can submit the jobs to edge compute nodes. In the IoT DevCloud, you can do this using the `qsub` command.\n",
    "We can submit benchmark_job to 10 different types of edge compute nodes simultaneously or just one node at at time.\n",
    "\n",
    "There are three options of `qsub` command that we use for this:\n",
    "- `-l` : this option lets us select the number and the type of nodes using `nodes={node_count}:{property}`. \n",
    "- `-F` : this option lets us send arguments to the bash script. \n",
    "- `-N` : this option lets use name the job so that it is easier to distinguish between them.\n",
    "\n",
    "The `-F` flag is used to pass in arguments to the job script.\n",
    "The [benchmark_job.sh](benchmark_job.sh) takes in 4 arguments:\n",
    "1. the path to the directory for the performance stats\n",
    "2. targeted device (e.g. CPU, GPU, MYRIAD, HDDL, FPGA)\n",
    "3. the floating precision to use for inference\n",
    "\n",
    "The job scheduler will use the contents of `-F` flag as the argument to the job script.\n",
    "\n",
    "If you are curious to see the available types of nodes on the IoT DevCloud, run the following optional cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pbsnodes | grep compnode | awk '{print $3}' | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the properties describe the node, and number on the left is the number of available nodes of that architecture.\n",
    "\n",
    "### 2.3 Job queue submission\n",
    "\n",
    "Each of the 5 cells below will submit a job to a different edge compute node.\n",
    "The output of the cell is the `JobID` of your job, which you can use to track progress of a job.\n",
    "\n",
    "**Note** You can submit all 5 jobs at once or follow one at a time. \n",
    "\n",
    "After submission, they will go into a queue and run as soon as the requested compute resources become available. \n",
    "(tip: **shift+enter** will run the cell and automatically move you to the next cell. So you can hit **shift+enter** multiple times to quickly run multiple cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with an Intel Core CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel \n",
    "    Core i5-6500TE</a>. The inference workload will run the CPU and Intel® HD Graphics 530 card integrated with the CPU. Both float and half-float precisions are covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit job to the queue\n",
    "job_id_core = !qsub benchmark_job.sh -l nodes=1:tank-870:i5-6500te -F \"results/ CPU FP32 4\" -N benchmark_core\n",
    "print(job_id_core[0]) \n",
    "\n",
    "# Progress indicators\n",
    "if job_id_core:\n",
    "    progressIndicator('results/', 'i_progress_' + job_id_core[0] + '.txt', \"Inference\", 0, 100)\n",
    "else:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit job to the queue\n",
    "job_id_gpu = !qsub benchmark_job.sh -l nodes=1:tank-870:i5-6500te:intel-hd-530 -F \"results/ GPU FP32 8\" -N benchmark_gpu \n",
    "print(job_id_gpu[0]) \n",
    "\n",
    "# Progress indicators\n",
    "if job_id_gpu:\n",
    "    progressIndicator('results/', 'i_progress_' + job_id_gpu[0] + '.txt', \"Inference\", 0, 100)\n",
    "else:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit job to the queue\n",
    "job_id_gpu_half = !qsub benchmark_job.sh -l nodes=1:tank-870:i5-6500te:intel-hd-530 -F \"results/ GPU FP16 8\" -N benchmark_gpu_half \n",
    "print(job_id_gpu_half[0]) \n",
    "\n",
    "# Progress indicators\n",
    "if job_id_gpu_half:\n",
    "    progressIndicator('results/', 'i_progress_' + job_id_gpu_half[0] + '.txt', \"Inference\", 0, 100)\n",
    "else:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel Xeon CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88178/Intel-Xeon-Processor-E3-1268L-v5-8M-Cache-2-40-GHz-\">Intel \n",
    "    Xeon Processor E3-1268L v5</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit job to the queue\n",
    "job_id_xeon = !qsub benchmark_job.sh -l nodes=1:tank-870:e3-1268l-v5 -F \"results/ CPU FP32 4\" -N benchmark_xeon \n",
    "print(job_id_xeon[0]) \n",
    "\n",
    "# Progress indicators\n",
    "if job_id_xeon:\n",
    "    progressIndicator('results/', 'i_progress_' + job_id_xeon[0] + '.txt', \"Inference\", 0, 100)\n",
    "else:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with High Density Deep Learning- FPGA (HDDL-F)\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500te CPU</a> . The inference workload will run on the <a href=\"https://www.ieiworld.com/mustang-f100/en/\"> IEI Mustang-F100-A10 </a> card installed in this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit job to the queue\n",
    "job_id_fpga = !qsub benchmark_job.sh -l nodes=1:tank-870:i5-6500te:hddl-f -F \"results/ HETERO:FPGA,CPU FP32 8\" -N benchmark_fpga\n",
    "print(job_id_fpga[0]) \n",
    "\n",
    "# Progress indicators\n",
    "if job_id_fpga:\n",
    "    progressIndicator('results/', 'i_progress_' + job_id_fpga[0] + '.txt', \"Inference\", 0, 100)\n",
    "else:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel Movidius NCS (Neural Compute Stick)\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500te CPU</a>. The inference workload will run on an <a \n",
    "    href=\"https://software.intel.com/en-us/movidius-ncs\">Intel \n",
    "    Movidius Neural Compute Stick</a> installed in this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit job to the queue\n",
    "job_id_ncs = !qsub benchmark_job.sh -l nodes=1:tank-870:i5-6500te:intel-ncs -F \"results/ MYRIAD FP16 8\" -N benchmark_ncs\n",
    "print(job_id_ncs[0]) \n",
    "\n",
    "# Progress indicators\n",
    "if job_id_ncs:\n",
    "    progressIndicator('results/', 'i_progress_' + job_id_ncs[0] + '.txt', \"Inference\", 0, 100)\n",
    "else:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel NCS 2 (Neural Compute Stick 2)\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500te CPU</a>. The inference workload will run on an <a \n",
    "    href=\"https://software.intel.com/en-us/neural-compute-stick\">Intel Neural Compute Stick 2</a> installed in this  node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit job to the queue\n",
    "job_id_ncs2 = !qsub benchmark_job.sh -l nodes=1:tank-870:i5-6500te:intel-ncs2 -F \"results/ MYRIAD FP16 32\" -N benchmark_ncs2\n",
    "print(job_id_ncs2[0]) \n",
    "\n",
    "# Progress indicators\n",
    "if job_id_ncs2:\n",
    "    progressIndicator('results/', 'i_progress_' + job_id_ncs2[0] + '.txt', \"Inference\", 0, 100)\n",
    "else:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with High Density Deep Learning- Myriad (HDDL-R)\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500te CPU</a>. The inference workload will run on an <a \n",
    "    href=\"https://www.ieiworld.com/mustang-v100/en/\">IEI Mustang-V100-MX8 </a>accelerator installed in this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit job to the queue\n",
    "job_id_hddlr = !qsub benchmark_job.sh -l nodes=1:tank-870:i5-6500te:iei-mustang-v100-mx8 -F \"results/ HDDL FP16 128\" -N benchmark_hddlr\n",
    "print(job_id_hddlr[0]) \n",
    "\n",
    "# Progress indicators\n",
    "if job_id_hddlr:\n",
    "    progressIndicator('results/', 'i_progress_' + job_id_hddlr[0] + '.txt', \"Inference\", 0, 100)\n",
    "else:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with UP Squared Grove IoT Development Kit (UP2)\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/up-squared-grove-dev-kit\">UP Squared Grove IoT Development Kit</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/96488/Intel-Atom-x7-E3950-Processor-2M-Cache-up-to-2-00-GHz-\">Intel Atom® x7-E3950 Processor</a>. The inference workload will run on the integrated Intel® HD Graphics 505 card, both full-precision and half-precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit job to the queue\n",
    "job_id_up2 = !qsub benchmark_job.sh -l nodes=1:up-squared -F \"results/ GPU FP32 8\" -N benchmark_up2\n",
    "print(job_id_up2[0]) \n",
    "\n",
    "# Progress indicators\n",
    "if job_id_up2:\n",
    "    progressIndicator('results/', 'i_progress_' + job_id_up2[0] + '.txt', \"Inference\", 0, 100)\n",
    "else:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit job to the queue\n",
    "job_id_up2_half = !qsub benchmark_job.sh -l nodes=1:up-squared -F \"results/ GPU FP16 8\" -N benchmark_up2_half\n",
    "print(job_id_up2_half[0]) \n",
    "\n",
    "# Progress indicators\n",
    "if job_id_up2_half:\n",
    "    progressIndicator('results/', 'i_progress_' + job_id_up2_half[0] + '.txt', \"Inference\", 0, 100)\n",
    "else:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Check if the jobs are done\n",
    "\n",
    "To check on the jobs that were submitted, use the `qstat` command.\n",
    "\n",
    "We have created a custom Jupyter widget  to get live qstat update.\n",
    "Run the following cell to bring it up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the jobs you have submitted (referenced by `Job ID` that gets displayed right after you submit the job in step 2.3).\n",
    "There should also be an extra job in the queue \"jupyterhub\": this job runs your current Jupyter Notebook session.\n",
    "\n",
    "The 'S' column shows the current status. \n",
    "- If it is in Q state, it is in the queue waiting for available resources. \n",
    "- If it is in R state, it is running. \n",
    "- If the job is no longer listed, it means it is completed.\n",
    "\n",
    "**Note**: Time spent in the queue depends on the number of users accessing the edge nodes. Once these jobs begin to run, they should take from 1 to 5 minutes to complete. \n",
    "\n",
    "***Wait!***\n",
    "\n",
    "Please wait for the inference jobs and video rendering complete before proceeding to the next step.\n",
    "\n",
    "## Step 3: View Results\n",
    "\n",
    "Once the jobs are completed, the queue system outputs the stdout and stderr streams of each job into files with names of the form\n",
    "\n",
    "`benchmark_{type}.o{JobID}`\n",
    "\n",
    "`benchmark_{type}.e{JobID}`\n",
    "\n",
    "(here, benchmark_{type} corresponds to the `-N` option of qsub).\n",
    "\n",
    "The running time of each inference task is recorded in `results/stats_job_id_{ARCH}.txt`, where the subdirectory name corresponds to the architecture of the target edge compute node. Run the cell below to plot the results of all jobs side-by-side. Higher values mean better performance. Keep in mind that some architectures are optimized for the highest performance, others for low power, as well as other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_dict = {'core':     'Intel Core\\ni5-6500TE\\nCPU',\n",
    "             'gpu':      'Intel Core\\ni5-6500TE\\nGPU@FP32',\n",
    "             'gpu_half': 'Intel Core\\ni5-6500TE\\nGPU@FP16',\n",
    "             'xeon':     'Intel Xeon\\nE3-1268L v5\\nCPU',\n",
    "             'ncs':      'Intel\\nMovidius\\nNCS',\n",
    "             'ncs2':     'Intel\\nMovidius\\nNCS2',\n",
    "             'hddlr':    'IEI Mustang\\nV100-MX8\\nHDDL-R',\n",
    "             'fpga':     'IEI Mustang\\nF100-A10\\nHDDL-F',\n",
    "             'up2':      'Intel Atom\\nx7-E3950\\nGPU@FP32',\n",
    "             'up2_half': 'Intel Atom\\nx7-E3950\\nGPU@FP16'}\n",
    "\n",
    "stats_dict = {}\n",
    "for arch, a_name in sorted(arch_dict.items()):\n",
    "    if 'job_id_' + arch in vars():\n",
    "        stats_dict['results/stats_' + vars()['job_id_' + arch][0] + '.txt'] = a_name\n",
    "    else:\n",
    "        stats_dict['placeholder' + arch] = a_name\n",
    "\n",
    "summaryPlot(stats_dict, 'Architecture', 'FPS', 'Inference Throughput', fontsize=12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
